{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Upsampling\n",
    "## Overview\n",
    "Hello! We are excited to share this take-home assignment with you. Please read the problem\n",
    "statement below and raise questions if any. We are happy to clarify over email or a quick call.\n",
    "Please try to get back to us in a week. After your solution is ready, please share it with us and\n",
    "schedule a 1-hour meeting to discuss your solution\n",
    "\n",
    "## Prompt\n",
    "At Connectly, ML engineers focus on both modeling and infra work. This task is designed to test\n",
    "your ability to design, build, train, and ship ML models\n",
    "\n",
    "## Task\n",
    "Your task is to build a model and go through the full ML lifecycle. You’re given a problem to\n",
    "solve with an ML model. You’ll need to take the model through the full ML lifecycle from ideation\n",
    "to deploymen\n",
    "\n",
    "## ML Problem\n",
    "We have taken 1536 Dimensional OpenAI embeddings and applied a mystery affine\n",
    "transformation to 32 dimensions. Your job is to build a model that given the 32-dimension\n",
    "embeddings can upsample them back to 1536-dimensions in a way that preserves the cosine\n",
    "angles between vectors of the original 1536-dimension embeddings. The upsampled vectors\n",
    "should be of unit length. This model is approximating the inverse of the mystery transform\n",
    "(mystery transform is not perfectly invertable)\n",
    "\n",
    "We will provide:\n",
    "\n",
    "1. Training Embeddings: This contains 12130 32-dimensional unit length embeddings\n",
    "representing as a matrix of shape (12130,32). File:\n",
    "https://cdn.connectly.ai/interview_prompts/ml_embedding_upsample/projected_train_embs.npy\n",
    "\n",
    "\n",
    "\n",
    "1. Cosine Angle Similarity Matrix: This is a (12130, 12130) where the value at position ij\n",
    "corresponds to the cosine angle <embedding_i, embedding_j>. Since these are unit\n",
    "vectors they were calculated as dot<embedding_i, embedding_j>. These were\n",
    "calculated on the original 1536-Dimensional embeddings NOT the 32-Dimensional\n",
    "projected embeddings. File:\n",
    "https://cdn.connectly.ai/interview_prompts/ml_embedding_upsample/og_train_cos_theta.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12130, 12130)\n",
      "(12130, 32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "og = np.load('og_train_cos_theta.npy')\n",
    "print(og.shape)\n",
    "\n",
    "projected = np.load('projected_train_embs.npy')\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32)\n",
      "(2130, 32)\n",
      "(10000, 10000)\n",
      "(2130, 2130)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "projected_train = projected[: 10000]\n",
    "projected_test = projected[10000: ]\n",
    "\n",
    "og_train = og[:10000, :10000]\n",
    "og_test = og[10000:, 10000: ]\n",
    "\n",
    "print(projected_train.shape)\n",
    "print(projected_test.shape)\n",
    "\n",
    "print(og_train.shape)\n",
    "print(og_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class FiveLayerNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FiveLayerNetwork, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(32, 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 128) # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 256) # Third hidden layer\n",
    "        self.fc4 = nn.Linear(256, 512) # Fourth hidden layer\n",
    "        self.fc5 = nn.Linear(512, 1536) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of the layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowwise_mse(matrix1, matrix2):\n",
    "    if matrix1.shape != matrix2.shape:\n",
    "        raise ValueError(\"Both matrices should have the same shape.\")\n",
    "    \n",
    "    matrix2 = torch.from_numpy(matrix2).to(matrix1.device)  # Convert to the same device as matrix1\n",
    "    mse_per_row = torch.mean((matrix1 - matrix2)**2, axis=1)\n",
    "    return mse_per_row\n",
    "\n",
    "def get_unit_vector(tensor):\n",
    "    return tensor / torch.norm(tensor, dim=1, keepdim=True)\n",
    "\n",
    "def custom_loss(upsampled_vectors, original_cosine_similarity_subset):\n",
    "    upsampled_cosine_similarity_matrix = F.cosine_similarity(upsampled_vectors.unsqueeze(0), upsampled_vectors.unsqueeze(1), dim=2)\n",
    "    cosine_similarity_loss = rowwise_mse(upsampled_cosine_similarity_matrix, original_cosine_similarity_subset)\n",
    "    return cosine_similarity_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_data = TensorDataset(torch.from_numpy(projected_train).float())\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=False)\n",
    "model = FiveLayerNetwork()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0007969019325098784\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_data = batch[0]\n",
    "        upsampled_vectors = model(input_data)\n",
    "        upsampled_vectors = get_unit_vector(upsampled_vectors)\n",
    "        \n",
    "        batch_indices = torch.arange(i * train_loader.len(input_data), (i + 1) * len(input_data))\n",
    "        \n",
    "        # Limit batch_indices to the actual size of og_train\n",
    "        batch_indices = torch.clamp(batch_indices, 0, len(og_train) - 1)\n",
    "\n",
    "        # Retrieve the subset of `og_train` using the batch indices\n",
    "        original_cosine_similarity_subset = og_train[batch_indices][:, batch_indices]\n",
    "        \n",
    "        row_loss = custom_loss(upsampled_vectors, original_cosine_similarity_subset)\n",
    "        loss = row_loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad\n",
    "\n",
    "# Testing (evaluation)\n",
    "\n",
    "# Forward pass on test data\n",
    "with no_grad():\n",
    "    upsampled_test_vectors = model(projected_test)\n",
    "    test_loss = custom_loss(upsampled_test_vectors, og_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
